{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88217d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = \"/home/ubuntu/IP-Adapter/output_dir/checkpoint-80-0/ip_adapter.bin\"\n",
    "sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "image_proj_sd = {}\n",
    "ip_sd = {}\n",
    "for k in sd:\n",
    "    if k.startswith(\"unet\"):\n",
    "        pass\n",
    "    elif k.startswith(\"image_proj_model\"):\n",
    "        image_proj_sd[k.replace(\"image_proj_model.\", \"\")] = sd[k]\n",
    "    elif k.startswith(\"adapter_modules\"):\n",
    "        ip_sd[k.replace(\"adapter_modules.\", \"\")] = sd[k]\n",
    "\n",
    "torch.save({\"image_proj\": image_proj_sd, \"ip_adapter\": ip_sd}, \"ip_adapter.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a329336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c59b3-f177-4a10-8925-d931ce572eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import MethodType\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, DDIMScheduler, AutoencoderKL, ControlNetModel\n",
    "from PIL import Image\n",
    "\n",
    "from ip_adapter import IPAdapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6dc69c-192d-4d74-8b1e-f0d9ccfbdb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"models/image_encoder/\"\n",
    "ip_ckpt = \"models/ip-adapter_sd15.bin\"\n",
    "# ip_ckpt = \"/home/ubuntu/IP-Adapter/output_dir_finetune/checkpoint-2-0/ip_adapter.bin\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec542f-8474-4f38-9457-073425578073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4baf0-eb72-47c6-8b5b-4046c7c7c72e",
   "metadata": {},
   "source": [
    "## ControlNet Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849f9d0-5f68-4a49-9190-69dd50720cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load controlnet\n",
    "controlnet_model_path = \"lllyasviel/control_v11f1p_sd15_depth\"\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16)\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09e937-3904-4d8e-a559-9066502ded36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image prompt\n",
    "image = Image.open(\"assets/images/statue.png\")\n",
    "depth_map = Image.open(\"assets/structure_controls/depth.png\")\n",
    "image_grid([image.resize((256, 256)), depth_map.resize((256, 256))], 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b1ab06-d3ed-4a7e-a356-9ddf1a2eecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ip-adapter\n",
    "ip_model = IPAdapter(pipe, image_encoder_path, ip_ckpt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd9556-988c-4a16-9408-17c8919d839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate image variations\n",
    "images = ip_model.generate(pil_image=image, image=depth_map, num_samples=4, num_inference_steps=50, seed=42)\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf199405-7cb5-4f78-9973-5fe51c632a41",
   "metadata": {},
   "source": [
    "## ControlNet OpenPose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f089ad0-4683-46d7-ab58-9e5fe8f34c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SD pipe\n",
    "del pipe, ip_model\n",
    "torch.cuda.empty_cache()\n",
    "# load controlnet\n",
    "controlnet_model_path = \"lllyasviel/control_v11p_sd15_openpose\"\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16)\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db2b55-2f56-4eef-b2ca-c5126b14feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image prompt\n",
    "# image = Image.open(\"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/adventure_girl/dead/frame_1.png\")\n",
    "image = Image.open(\"/home/ubuntu/IP-Adapter/data/Test/in_sample/Ash Williams/motions/stand/reference.png\")\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d1e25-5b50-4d2e-851d-4ba620a55ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_image = Image.open(\"/home/ubuntu/IP-Adapter/data/Test/in_sample/Ash Williams/motions/stand/poses/humanpose_3.png\")\n",
    "# openpose_image = Image.open(\"assets/structure_controls/openpose.png\")\n",
    "openpose_image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501f284-f295-4673-96ab-e34378da62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ip-adapter\n",
    "ip_model = IPAdapter(pipe, image_encoder_path, ip_ckpt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a93ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "images = ip_model.generate(pil_image=image, image=openpose_image, width=512, height=512, num_samples=4, num_inference_steps=100, seed=42)\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37777c10",
   "metadata": {},
   "source": [
    "Generate without finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Image grid helper function\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows * cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols * w, rows * h))\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n",
    "# Paths\n",
    "ref_image_path = \"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/ninjagirlnew/throw/frame_1.png\"\n",
    "pose_image_paths = [\n",
    "    \"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/ninjagirlnew/throw/humanpose_3.png\",\n",
    "    \"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/ninjagirlnew/throw/humanpose_4.png\",\n",
    "    \"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/ninjagirlnew/throw/humanpose_5.png\",\n",
    "]\n",
    "\n",
    "# Generate corresponding GT paths by replacing 'humanpose' with 'frame' in the pose paths\n",
    "gt_image_paths = [pose_path.replace(\"humanpose\", \"frame\") for pose_path in pose_image_paths]\n",
    "\n",
    "# Load and preprocess reference image\n",
    "ref_image = Image.open(ref_image_path).resize((512, 512))\n",
    "\n",
    "# Generate images for each pose\n",
    "generated_images = []\n",
    "pose_images = []\n",
    "gt_images = []\n",
    "\n",
    "for pose_path, gt_path in zip(pose_image_paths, gt_image_paths):\n",
    "    pose_image = Image.open(pose_path).resize((512, 512))\n",
    "    gt_image = Image.open(gt_path).resize((512, 512))  # Load and resize GT image\n",
    "    generated_image = ip_model.generate(\n",
    "        pil_image=ref_image,\n",
    "        image=pose_image,\n",
    "        width=512,\n",
    "        height=512,\n",
    "        num_samples=1,  # Single output per pose image\n",
    "        num_inference_steps=100,\n",
    "        seed=42\n",
    "    )[0]  # Access the first (and only) generated sample\n",
    "    pose_images.append(pose_image)\n",
    "    generated_images.append(generated_image)\n",
    "    gt_images.append(gt_image)\n",
    "\n",
    "# Create visualization grid\n",
    "rows = len(pose_images)  # Each row contains ref, pose, generated, and GT images\n",
    "cols = 4  # Four columns: ref, pose, generated, GT\n",
    "all_images = []\n",
    "\n",
    "# Add images row-wise: ref, pose, generated, GT\n",
    "for pose_image, generated_image, gt_image in zip(pose_images, generated_images, gt_images):\n",
    "    all_images.extend([ref_image, pose_image, generated_image, gt_image])\n",
    "\n",
    "# Ensure the grid dimensions match the images count\n",
    "assert len(all_images) == rows * cols, (\n",
    "    f\"Number of images ({len(all_images)}) does not match grid size ({rows}x{cols}).\"\n",
    ")\n",
    "\n",
    "# Create and save the grid\n",
    "grid = image_grid(all_images, rows, cols)\n",
    "grid.save(\"output_grid_with_gt.png\")\n",
    "grid.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d11fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Helper function to concatenate images horizontally\n",
    "def horizontal_concat(images, resize_to=None):\n",
    "    widths, heights = zip(*(img.size for img in images))\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "\n",
    "    if resize_to:\n",
    "        images = [img.resize(resize_to) for img in images]\n",
    "\n",
    "    concat_image = Image.new('RGB', (total_width, max_height))\n",
    "    x_offset = 0\n",
    "    for img in images:\n",
    "        concat_image.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "    return concat_image\n",
    "\n",
    "# Paths\n",
    "ref_image_path = \"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/ninjagirlnew/throw/frame_1.png\"\n",
    "pose_image_paths = [\n",
    "    \"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/ninjagirlnew/throw/humanpose_3.png\",\n",
    "    \"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/ninjagirlnew/throw/humanpose_4.png\",\n",
    "    \"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/ninjagirlnew/throw/humanpose_5.png\",\n",
    "]\n",
    "\n",
    "# Load and preprocess reference image\n",
    "ref_image = Image.open(ref_image_path).resize((512, 512))\n",
    "\n",
    "# Generate corresponding GT paths by replacing 'humanpose' with 'frame' in the pose paths\n",
    "gt_image_paths = [pose_path.replace(\"humanpose\", \"frame\") for pose_path in pose_image_paths]\n",
    "\n",
    "# Load pose and GT images\n",
    "pose_images = [Image.open(pose_path).resize((512, 512)) for pose_path in pose_image_paths]\n",
    "gt_images = [Image.open(gt_path).resize((512, 512)) for gt_path in gt_image_paths]\n",
    "\n",
    "# Concatenate all pose images into one horizontal strip\n",
    "concat_pose_image = horizontal_concat(pose_images)\n",
    "\n",
    "# Generate the image with concatenated poses\n",
    "generated_image = ip_model.generate(\n",
    "    pil_image=ref_image,\n",
    "    image=concat_pose_image,\n",
    "    width=512 * len(pose_images),  # Width matches concatenated poses\n",
    "    height=512,\n",
    "    num_samples=1,  # Single output\n",
    "    num_inference_steps=100,\n",
    "    seed=42\n",
    ")[0]\n",
    "\n",
    "# Concatenate GT images into one horizontal strip\n",
    "concat_gt_image = horizontal_concat(gt_images)\n",
    "\n",
    "# Prepare visualization\n",
    "# First row: Repeat ref_image 4 times horizontally\n",
    "ref_row = horizontal_concat([ref_image] * len(pose_images))\n",
    "\n",
    "# Second row: Concatenated pose images\n",
    "pose_row = concat_pose_image\n",
    "\n",
    "# Third row: Generated image (already single concatenated image)\n",
    "generated_row = generated_image\n",
    "\n",
    "# Fourth row: Concatenated GT images\n",
    "gt_row = concat_gt_image\n",
    "\n",
    "# Combine all rows vertically\n",
    "final_grid = Image.new('RGB', (ref_row.width, ref_row.height * 4))\n",
    "final_grid.paste(ref_row, (0, 0))\n",
    "final_grid.paste(pose_row, (0, ref_row.height))\n",
    "final_grid.paste(generated_row, (0, ref_row.height * 2))\n",
    "final_grid.paste(gt_row, (0, ref_row.height * 3))\n",
    "\n",
    "# Save and display the final grid\n",
    "final_grid.save(\"output_grid_with_gt_4x1.png\")\n",
    "final_grid.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a4afd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761fc314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from PIL import Image\n",
    "# from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "# from ip_adapter import IPAdapter  # Replace with your actual IPAdapter import\n",
    "\n",
    "# # Helper function to create an image grid\n",
    "# def image_grid(imgs, rows, cols):\n",
    "#     assert len(imgs) == rows * cols\n",
    "\n",
    "#     w, h = imgs[0].size\n",
    "#     grid = Image.new('RGB', size=(cols * w, rows * h))\n",
    "#     for i, img in enumerate(imgs):\n",
    "#         grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "#     return grid\n",
    "\n",
    "# # Paths\n",
    "# ref_image_path = \"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/adventure_girl/jump/frame_1.png\"\n",
    "# pose_image_paths = [\n",
    "#     \"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/adventure_girl/jump/humanpose_2.png\",\n",
    "#     \"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/adventure_girl/jump/humanpose_3.png\",\n",
    "#     \"/home/ubuntu/Sprite-Sheet-Diffusion/data_handlabel/adventure_girl/jump/humanpose_4.png\",\n",
    "# ]\n",
    "\n",
    "# # Model configuration\n",
    "# base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "# vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "# controlnet_model_path = \"lllyasviel/control_v11p_sd15_openpose\"\n",
    "# image_encoder_path = \"models/image_encoder/\"\n",
    "# ip_ckpt = \"models/ip-adapter_sd15.bin\"\n",
    "# device = \"cuda\"\n",
    "\n",
    "# # Load models\n",
    "# torch.cuda.empty_cache()\n",
    "# controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16).to(device)\n",
    "# vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)\n",
    "# pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "#     base_model_path,\n",
    "#     controlnet=controlnet,\n",
    "#     vae=vae,\n",
    "#     torch_dtype=torch.float16\n",
    "# ).to(device)\n",
    "\n",
    "# # Adjust ControlNet influence\n",
    "# pipe.controlnet_conditioning_scale = 0.8  # Default is 1.0; reduce for looser pose adherence\n",
    "\n",
    "# # Initialize IPAdapter\n",
    "# ip_model = IPAdapter(pipe, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "# # Load and preprocess reference image\n",
    "# ref_image = Image.open(ref_image_path).resize((512, 512))\n",
    "\n",
    "# # Generate images for each pose\n",
    "# generated_images = []\n",
    "# pose_images = []\n",
    "# for pose_path in pose_image_paths:\n",
    "#     pose_image = Image.open(pose_path).resize((512, 512))\n",
    "#     generated_image = ip_model.generate(\n",
    "#         pil_image=ref_image,\n",
    "#         image=pose_image,\n",
    "#         width=512,\n",
    "#         height=512,\n",
    "#         num_samples=1,  # Single output per pose image\n",
    "#         num_inference_steps=150,  # Increase steps for better quality\n",
    "#         seed=42,\n",
    "#         # adapter_weight=0.8,  # Increase IPAdapter influence\n",
    "#         guidance_scale=10.0  # Adjust guidance scale for stricter adherence to reference\n",
    "#     )[0]  # Access the first (and only) generated sample\n",
    "#     pose_images.append(pose_image)\n",
    "#     generated_images.append(generated_image)\n",
    "\n",
    "# # Create visualization grid\n",
    "# rows = len(pose_images)  # Each row contains ref, pose, and generated images\n",
    "# cols = 3  # One column each for ref, pose, and generated image\n",
    "# all_images = []\n",
    "\n",
    "# # Add images row-wise: ref, pose, generated\n",
    "# for pose_image, generated_image in zip(pose_images, generated_images):\n",
    "#     all_images.extend([ref_image, pose_image, generated_image])\n",
    "\n",
    "# # Ensure the grid dimensions match the images count\n",
    "# assert len(all_images) == rows * cols, (\n",
    "#     f\"Number of images ({len(all_images)}) does not match grid size ({rows}x{cols}).\"\n",
    "# )\n",
    "\n",
    "# # Create and save the grid\n",
    "# grid = image_grid(all_images, rows, cols)\n",
    "# grid.save(\"output_grid.png\")\n",
    "# grid.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f2ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb587d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all, first frame as reference, generate one frame at a time\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Helper function to concatenate images horizontally\n",
    "def horizontal_concat(images, resize_to=None):\n",
    "    widths, heights = zip(*(img.size for img in images))\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "\n",
    "    if resize_to:\n",
    "        images = [img.resize(resize_to) for img in images]\n",
    "\n",
    "    concat_image = Image.new('RGB', (total_width, max_height))\n",
    "    x_offset = 0\n",
    "    for img in images:\n",
    "        concat_image.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "    return concat_image\n",
    "\n",
    "# Paths\n",
    "base_path = \"/home/ubuntu/ssd_high_quality/test/characters\"\n",
    "output_folder_name = \"ipadaptor\"\n",
    "resize_dim = (512, 512)\n",
    "\n",
    "# Loop through each character folder\n",
    "for character_name in os.listdir(base_path):\n",
    "    character_path = os.path.join(base_path, character_name)\n",
    "    if not os.path.isdir(character_path):\n",
    "        continue\n",
    "    \n",
    "    # Loop through each motion folder\n",
    "    motions_path = os.path.join(character_path, \"motions\")\n",
    "    for motion_name in os.listdir(motions_path):\n",
    "        motion_path = os.path.join(motions_path, motion_name)\n",
    "        if not os.path.isdir(motion_path):\n",
    "            continue\n",
    "        \n",
    "        # Paths for ground_truth, poses, and output folders\n",
    "        ground_truth_path = os.path.join(motion_path, \"ground_truth\")\n",
    "        poses_path = os.path.join(motion_path, \"poses\")\n",
    "        output_path = os.path.join(motion_path, output_folder_name)\n",
    "\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Load reference image (first frame in ground_truth)\n",
    "        ref_image_path = os.path.join(ground_truth_path, \"frame_1.png\")\n",
    "        if not os.path.exists(ref_image_path):\n",
    "            print(f\"Reference image not found: {ref_image_path}\")\n",
    "            continue\n",
    "        ref_image = Image.open(ref_image_path).resize(resize_dim)\n",
    "\n",
    "        # Loop through pose images in the poses folder\n",
    "        for pose_image_name in sorted(os.listdir(poses_path)):\n",
    "            if not pose_image_name.endswith(\".png\"):\n",
    "                continue\n",
    "\n",
    "            pose_image_path = os.path.join(poses_path, pose_image_name)\n",
    "            pose_image = Image.open(pose_image_path).resize(resize_dim)\n",
    "\n",
    "            # Generate prediction using ipadaptormodel\n",
    "            generated_image = ip_model.generate(\n",
    "                pil_image=ref_image,\n",
    "                image=pose_image,\n",
    "                width=resize_dim[0],\n",
    "                height=resize_dim[1],\n",
    "                num_samples=1,\n",
    "                num_inference_steps=100,\n",
    "                seed=42\n",
    "            )[0]\n",
    "\n",
    "            # Save the output image\n",
    "            output_image_name = pose_image_name.replace(\"humanpose\", \"generated\")\n",
    "            output_image_path = os.path.join(output_path, output_image_name)\n",
    "            generated_image.save(output_image_path)\n",
    "\n",
    "        print(f\"Processed motion: {motion_name} for character: {character_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f787bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all, first frame as reference, generate all frames at once\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Helper function to concatenate images horizontally\n",
    "def horizontal_concat(images, resize_to=None):\n",
    "    widths, heights = zip(*(img.size for img in images))\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "\n",
    "    if resize_to:\n",
    "        images = [img.resize(resize_to) for img in images]\n",
    "\n",
    "    concat_image = Image.new('RGB', (total_width, max_height))\n",
    "    x_offset = 0\n",
    "    for img in images:\n",
    "        concat_image.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "    return concat_image\n",
    "\n",
    "# Helper function to split a concatenated image back into frames\n",
    "def split_horizontal_image(image, num_frames, frame_width=512, frame_height=512):\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        left = i * frame_width\n",
    "        right = left + frame_width\n",
    "        frame = image.crop((left, 0, right, frame_height))\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "# Paths\n",
    "base_path = \"/home/ubuntu/ssd_high_quality/test/characters\"\n",
    "output_folder_name = \"ipadaptor_combinedgenerate\"\n",
    "resize_dim = (512, 512)\n",
    "\n",
    "# Loop through each character folder\n",
    "for character_name in os.listdir(base_path):\n",
    "    character_path = os.path.join(base_path, character_name)\n",
    "    if not os.path.isdir(character_path):\n",
    "        continue\n",
    "    \n",
    "    # Loop through each motion folder\n",
    "    motions_path = os.path.join(character_path, \"motions\")\n",
    "    for motion_name in os.listdir(motions_path):\n",
    "        motion_path = os.path.join(motions_path, motion_name)\n",
    "        if not os.path.isdir(motion_path):\n",
    "            continue\n",
    "        \n",
    "        # Paths for ground_truth, poses, and output folders\n",
    "        ground_truth_path = os.path.join(motion_path, \"ground_truth\")\n",
    "        poses_path = os.path.join(motion_path, \"poses\")\n",
    "        output_path = os.path.join(motion_path, output_folder_name)\n",
    "\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Load reference image (first frame in ground_truth)\n",
    "        ref_image_path = os.path.join(ground_truth_path, \"frame_1.png\")\n",
    "        if not os.path.exists(ref_image_path):\n",
    "            print(f\"Reference image not found: {ref_image_path}\")\n",
    "            continue\n",
    "        ref_image = Image.open(ref_image_path).resize(resize_dim)\n",
    "\n",
    "        # Load all pose images and concatenate them\n",
    "        pose_images = []\n",
    "        for pose_image_name in sorted(os.listdir(poses_path)):\n",
    "            if not pose_image_name.endswith(\".png\"):\n",
    "                continue\n",
    "            pose_image_path = os.path.join(poses_path, pose_image_name)\n",
    "            pose_images.append(Image.open(pose_image_path).resize(resize_dim))\n",
    "\n",
    "        if not pose_images:\n",
    "            print(f\"No pose images found in: {poses_path}\")\n",
    "            continue\n",
    "\n",
    "        concat_pose_image = horizontal_concat(pose_images)\n",
    "\n",
    "        # Generate combined output using ipadaptor\n",
    "        generated_image = ip_model.generate(\n",
    "            pil_image=ref_image,\n",
    "            image=concat_pose_image,\n",
    "            width=concat_pose_image.width,  # Total width of the concatenated image\n",
    "            height=concat_pose_image.height,  # Height remains 512\n",
    "            num_samples=1,\n",
    "            num_inference_steps=100,\n",
    "            seed=42\n",
    "        )[0]\n",
    "\n",
    "        # Split the generated image back into individual frames\n",
    "        num_frames = len(pose_images)\n",
    "        generated_frames = split_horizontal_image(\n",
    "            generated_image,\n",
    "            num_frames=num_frames,\n",
    "            frame_width=resize_dim[0],\n",
    "            frame_height=resize_dim[1]\n",
    "        )\n",
    "\n",
    "        # Save each frame to the output folder\n",
    "        for i, frame in enumerate(generated_frames, start=1):\n",
    "            output_image_path = os.path.join(output_path, f\"generated_{i}.png\")\n",
    "            frame.save(output_image_path)\n",
    "\n",
    "        print(f\"Processed motion: {motion_name} for character: {character_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228de2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc9bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603426a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Helper function to concatenate images horizontally\n",
    "def horizontal_concat(images, resize_to=None):\n",
    "    widths, heights = zip(*(img.size for img in images))\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "\n",
    "    if resize_to:\n",
    "        images = [img.resize(resize_to) for img in images]\n",
    "\n",
    "    concat_image = Image.new('RGB', (total_width, max_height))\n",
    "    x_offset = 0\n",
    "    for img in images:\n",
    "        concat_image.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "    return concat_image\n",
    "\n",
    "# Helper function to split a concatenated image back into frames\n",
    "def split_horizontal_image(image, num_frames, frame_width=512, frame_height=512):\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        left = i * frame_width\n",
    "        right = left + frame_width\n",
    "        frame = image.crop((left, 0, right, frame_height))\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "# Base paths\n",
    "base_path = \"/home/ubuntu/IP-Adapter/data/Test/in_sample/adventure_girl/motions/dead\"\n",
    "resize_dim = (512, 512)\n",
    "output_folder_name = \"predict\"\n",
    "\n",
    "# Paths for ground_truth, poses, and output\n",
    "ground_truth_path = os.path.join(base_path, \"ground_truth\")\n",
    "poses_path = os.path.join(base_path, \"poses\")\n",
    "output_path = os.path.join(base_path, output_folder_name)\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Check if reference.png exists\n",
    "reference_image_path = os.path.join(base_path, \"reference.png\")\n",
    "if os.path.exists(reference_image_path):\n",
    "    ref_image = Image.open(reference_image_path).resize(resize_dim)\n",
    "else:\n",
    "    # Use the first frame in ground_truth as the reference image\n",
    "    ground_truth_images = sorted([f for f in os.listdir(ground_truth_path) if f.endswith(\".png\")])\n",
    "    if not ground_truth_images:\n",
    "        raise ValueError(f\"No images found in ground_truth folder: {ground_truth_path}\")\n",
    "    reference_image_path = os.path.join(ground_truth_path, ground_truth_images[0])\n",
    "    ref_image = Image.open(reference_image_path).resize(resize_dim)\n",
    "\n",
    "# Load all pose images and concatenate them\n",
    "pose_images = []\n",
    "for pose_image_name in sorted(os.listdir(poses_path)):\n",
    "    if not pose_image_name.endswith(\".png\"):\n",
    "        continue\n",
    "    pose_image_path = os.path.join(poses_path, pose_image_name)\n",
    "    pose_images.append(Image.open(pose_image_path).resize(resize_dim))\n",
    "\n",
    "if not pose_images:\n",
    "    raise ValueError(f\"No pose images found in folder: {poses_path}\")\n",
    "\n",
    "# Concatenate pose images horizontally\n",
    "concat_pose_image = horizontal_concat(pose_images)\n",
    "\n",
    "# Use the model to generate the combined output\n",
    "generated_image = ip_model.generate(\n",
    "    pil_image=ref_image,\n",
    "    image=concat_pose_image,\n",
    "    width=concat_pose_image.width,  # Total width of the concatenated image\n",
    "    height=concat_pose_image.height,  # Height remains the same as individual images\n",
    "    num_samples=1,\n",
    "    num_inference_steps=100,\n",
    "    seed=42\n",
    ")[0]\n",
    "\n",
    "# Split the generated image back into individual frames\n",
    "num_frames = len(pose_images)\n",
    "generated_frames = split_horizontal_image(\n",
    "    generated_image,\n",
    "    num_frames=num_frames,\n",
    "    frame_width=resize_dim[0],\n",
    "    frame_height=resize_dim[1]\n",
    ")\n",
    "\n",
    "# Save each frame to the output folder\n",
    "for i, frame in enumerate(generated_frames, start=1):\n",
    "    output_image_path = os.path.join(output_path, f\"predict_{i}.png\")\n",
    "    frame.save(output_image_path)\n",
    "\n",
    "print(f\"Processed motion: dead\")\n",
    "print(f\"Generated images saved in: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d2270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "# Helper function for natural sort\n",
    "def natural_key(string):\n",
    "    \"\"\"Split string into chunks of numbers and text for natural sorting.\"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', string)]\n",
    "\n",
    "# Helper function to concatenate images horizontally\n",
    "def horizontal_concat(images, resize_to=None):\n",
    "    widths, heights = zip(*(img.size for img in images))\n",
    "    total_width = sum(widths)\n",
    "    max_height = max(heights)\n",
    "\n",
    "    if resize_to:\n",
    "        images = [img.resize(resize_to) for img in images]\n",
    "\n",
    "    concat_image = Image.new('RGB', (total_width, max_height))\n",
    "    x_offset = 0\n",
    "    for img in images:\n",
    "        concat_image.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "    return concat_image\n",
    "\n",
    "# Helper function to split a concatenated image back into frames\n",
    "def split_horizontal_image(image, num_frames, frame_width=512, frame_height=512):\n",
    "    frames = []\n",
    "    for i in range(num_frames):\n",
    "        left = i * frame_width\n",
    "        right = left + frame_width\n",
    "        frame = image.crop((left, 0, right, frame_height))\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "# Base paths\n",
    "base_paths = [\n",
    "    \"/home/ubuntu/IP-Adapter/data/Test/in_sample\",\n",
    "    \"/home/ubuntu/IP-Adapter/data/Test/out_sample\"\n",
    "]\n",
    "resize_dim = (512, 512)\n",
    "output_folder_name = \"predict\"\n",
    "\n",
    "# Loop through each base path (in_sample and out_sample)\n",
    "for base_path in base_paths:\n",
    "    for character_name in os.listdir(base_path):\n",
    "        character_path = os.path.join(base_path, character_name)\n",
    "        if not os.path.isdir(character_path):\n",
    "            continue\n",
    "        \n",
    "        # Loop through each motion folder\n",
    "        motions_path = os.path.join(character_path, \"motions\")\n",
    "        for motion_name in os.listdir(motions_path):\n",
    "            motion_path = os.path.join(motions_path, motion_name)\n",
    "            if not os.path.isdir(motion_path):\n",
    "                continue\n",
    "            \n",
    "            # Paths for ground_truth, poses, and output\n",
    "            ground_truth_path = os.path.join(motion_path, \"ground_truth\")\n",
    "            poses_path = os.path.join(motion_path, \"poses\")\n",
    "            output_path = os.path.join(motion_path, output_folder_name)\n",
    "\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "            # 检查 predict 文件夹是否存在，并判断是否需要跳过生成步骤\n",
    "            if not os.path.exists(output_path):\n",
    "                os.makedirs(output_path, exist_ok=True)\n",
    "                print(f\"Creating predict folder for motion: {motion_name} for character: {character_name}\")\n",
    "            else:\n",
    "                # 检查 predict 文件夹中是否已生成所有预测帧\n",
    "                expected_frames = len(os.listdir(poses_path))\n",
    "                existing_predict_frames = [\n",
    "                    f for f in os.listdir(output_path) if f.startswith(\"predict_\") and f.endswith(\".png\")\n",
    "                ]\n",
    "                if len(existing_predict_frames) == expected_frames:\n",
    "                    print(f\"Skipping motion: {motion_name} for character: {character_name} (already processed)\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "            # Check if reference.png exists\n",
    "            reference_image_path = os.path.join(motion_path, \"reference.png\")\n",
    "            if os.path.exists(reference_image_path):\n",
    "                ref_image = Image.open(reference_image_path).resize(resize_dim)\n",
    "            else:\n",
    "                # Use the first frame in ground_truth as the reference image\n",
    "                ground_truth_images = sorted(\n",
    "                    [f for f in os.listdir(ground_truth_path) if f.endswith(\".png\")],\n",
    "                    key=natural_key  # Use natural sorting here\n",
    "                )\n",
    "                if not ground_truth_images:\n",
    "                    print(f\"No ground_truth images found in: {ground_truth_path}\")\n",
    "                    continue\n",
    "                reference_image_path = os.path.join(ground_truth_path, ground_truth_images[0])\n",
    "                ref_image = Image.open(reference_image_path).resize(resize_dim)\n",
    "\n",
    "            # Load all pose images\n",
    "            pose_images = sorted(\n",
    "                [f for f in os.listdir(poses_path) if f.endswith(\".png\")],\n",
    "                key=natural_key  # Use natural sorting here\n",
    "            )\n",
    "            pose_images = [Image.open(os.path.join(poses_path, img)).resize(resize_dim) for img in pose_images]\n",
    "\n",
    "            if not pose_images:\n",
    "                print(f\"No pose images found in: {poses_path}\")\n",
    "                continue\n",
    "\n",
    "            # Concatenate pose images horizontally\n",
    "            concat_pose_image = horizontal_concat(pose_images)\n",
    "\n",
    "            # Use the model to generate the combined output\n",
    "            generated_image = ip_model.generate(\n",
    "                pil_image=ref_image,\n",
    "                image=concat_pose_image,\n",
    "                width=concat_pose_image.width,  # Total width of the concatenated image\n",
    "                height=concat_pose_image.height,  # Height remains 512\n",
    "                num_samples=1,\n",
    "                num_inference_steps=100,\n",
    "                seed=42\n",
    "            )[0]\n",
    "\n",
    "            # Split the generated image back into individual frames\n",
    "            num_frames = len(pose_images)\n",
    "            generated_frames = split_horizontal_image(\n",
    "                generated_image,\n",
    "                num_frames=num_frames,\n",
    "                frame_width=resize_dim[0],\n",
    "                frame_height=resize_dim[1]\n",
    "            )\n",
    "\n",
    "            # Save each frame to the output folder\n",
    "            for i, frame in enumerate(generated_frames, start=1):\n",
    "                output_image_path = os.path.join(output_path, f\"predict_{i}.png\")\n",
    "                frame.save(output_image_path)\n",
    "\n",
    "            print(f\"Processed motion: {motion_name} for character: {character_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c74cd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
